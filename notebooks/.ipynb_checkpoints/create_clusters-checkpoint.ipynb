{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from annette.db import SessionManager\n",
    "from annette.db.models import Citation, ManualClassification\n",
    "import dill\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generic function to get a list of `{'attribute_name': attribute, 'class': classification}` records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(attr_name):\n",
    "    attr = getattr(Citation, attr_name)\n",
    "\n",
    "    with SessionManager() as session_manager:\n",
    "        citations = session_manager.session.query(attr,\n",
    "                                                  ManualClassification.classification_id) \\\n",
    "            .join(ManualClassification, Citation.doi == ManualClassification.doi) \\\n",
    "            .group_by(attr, ManualClassification.classification_id).all()\n",
    "\n",
    "    return [{\n",
    "        attr_name: getattr(c, attr_name) if getattr(c, attr_name) is not None else '',\n",
    "        'class': c.classification_id\n",
    "        } for c in citations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two tokenisers for the `subject` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def tokenise_subjects_simple(subject):\n",
    "    if subject == '' or subject is None:\n",
    "        return []\n",
    "    return list(set([s.strip().lower() for s in subject.split(',')]))\n",
    "\n",
    "\n",
    "def tokenise_subjects_phrases(subject):\n",
    "    if subject == '' or subject is None:\n",
    "        return []\n",
    "    split_subjects = []\n",
    "    phrase_pattern = 'CHUNK:{<JJ>*<NN.?>*<VBG>*}'\n",
    "    phrase_chunker = RegexpParser(phrase_pattern)\n",
    "    for s in subject.split(','):\n",
    "        tokens = word_tokenize(s.strip().lower())\n",
    "        tags = pos_tag(tokens)\n",
    "        phrases = [' '.join([leaf[0] for leaf in c.leaves()]) for c in phrase_chunker.parse(tags) if hasattr(c, 'label') and c.label() == 'CHUNK']\n",
    "        for phrase in phrases:\n",
    "            phrase_tokens = word_tokenize(phrase)\n",
    "            phrase_tags = pos_tag(phrase_tokens)\n",
    "            lemmatised_phrase = []\n",
    "            for pto, pta in phrase_tags:\n",
    "                wn_tag = {\n",
    "                    'n': wn.NOUN,\n",
    "                    'j': wn.ADJ,\n",
    "                    'v': wn.VERB,\n",
    "                    'r': wn.ADV\n",
    "                    }.get(pta[0].lower(), None)\n",
    "                if wn_tag is None:\n",
    "                    continue\n",
    "                lemmatised = WordNetLemmatizer().lemmatize(pto, wn_tag)\n",
    "                lemmatised_phrase.append(lemmatised)\n",
    "            if len(lemmatised_phrase) > 0:\n",
    "                lemmatised_phrase = ' '.join(lemmatised_phrase)\n",
    "                split_subjects.append(lemmatised_phrase)\n",
    "    return list(set(split_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple tokeniser just splits the string by commas (and converts to lowercase). The tags should be fairly standardised, so this shouldn't be too much of a problem.\n",
    "\n",
    "Just to see if it's more accurate/helpful to process the tags in a different way, we've also defined the 'phrase' tokeniser, which goes through several steps:\n",
    "1. Split the whole string by commas (assumes the field is a comma-separated list of phrasal tags);\n",
    "2. Tokenise the phrasal subject tags and identify parts of speech;\n",
    "3. Extract sub-phrases based on those parts of speech (i.e. adjective-noun-verb combinations, ignoring conjunctives like *and*);\n",
    "4. Lemmatise the words in those phrases to reduce differences between related tags (e.g. science and sciences both become *science*);\n",
    "5. Join the words back together to form a lemmatised sub-phrase.\n",
    "\n",
    "The two tokenisers will generate different length lists from the same input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nature and landscape conservation', 'ecology', 'aquatic science']\n",
      "['landscape conservation', 'ecology', 'nature', 'aquatic science']\n",
      "['behavior and systematics', 'nature and landscape conservation', 'ecology', 'evolution']\n",
      "['nature', 'landscape conservation', 'systematics', 'behavior', 'evolution', 'ecology']\n"
     ]
    }
   ],
   "source": [
    "print(tokenise_subjects_simple('Ecology,Aquatic Science,Nature and Landscape Conservation'))\n",
    "print(tokenise_subjects_phrases('Ecology,Aquatic Science,Nature and Landscape Conservation'))\n",
    "\n",
    "print(tokenise_subjects_simple('Ecology,Ecology, Evolution, Behavior and Systematics,Nature and Landscape Conservation'))\n",
    "print(tokenise_subjects_phrases('Ecology,Ecology, Evolution, Behavior and Systematics,Nature and Landscape Conservation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of records for training + testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records = get_data('subject')\n",
    "records_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tokenisation processes will otherwise follow the same transformation pipeline in order to extract the train/test data, so we can define a method that takes the tokeniser as an argument to avoid repeating ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def transform_data(tokeniser):\n",
    "    \n",
    "    # a list of all possible tags\n",
    "    all_items = list(set([item for r in records for item in tokeniser(r['subject'])]))\n",
    "    print(len(all_items))\n",
    "    \n",
    "    # define a binariser and fit it to the list of tags\n",
    "    binariser = MultiLabelBinarizer()\n",
    "    binariser.fit([all_items])\n",
    "    \n",
    "    # transform all the subject tag data\n",
    "    all_x = binariser.transform([tokeniser(s) for s in records_df['subject']])\n",
    "    print(records_df['subject'].to_numpy()[1])\n",
    "    print(all_x[1])\n",
    "    \n",
    "    # split into training and test data\n",
    "    train_x, test_x, train_y, test_y = train_test_split(all_x, records_df['class'],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=123, stratify=records_df['class'])\n",
    "    \n",
    "    return {\n",
    "               'x': train_x,\n",
    "               'y': train_y\n",
    "           }, {\n",
    "               'x': test_x,\n",
    "               'y': test_y\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a list of all the possible tags so that we can binarise them (convert them to a numeric representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "Ecology,Aquatic Science,Nature and Landscape Conservation\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_simple, test_simple = transform_data(tokenise_subjects_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "Ecology,Aquatic Science,Nature and Landscape Conservation\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_phrases, test_phrases = transform_data(tokenise_subjects_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the KMeans cluster models. Again, they'll follow the same pipeline, so it's easier to define a generic method that takes the train/test data as an argument.\n",
    "\n",
    "Since this is *unsupervised* learning, it cannot be scored for accuracy until it's combined with the *supervised* methods. We may need to generate several models with different settings (e.g. for `n_clusters`) to fine-tune the model at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def create_model(train, n_clusters=2):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    model.fit(train['x'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = create_model(train_simple)\n",
    "model_phrases = create_model(train_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model can be saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'kmeans_model_subject.pk'\n",
    "with open(filename, 'wb') as f:\n",
    "    dill.dump(model_simple, f)\n",
    "    \n",
    "\n",
    "# then to load again:\n",
    "with open(filename, 'rb') as f:\n",
    "    loaded_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans models are used to give a numerical representation to the list of tags. This can then be included in the training data (along with other features) for supervised learning techniques, e.g. Random Forest classifiers, Naive Bayes, and/or Neural Nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
