{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annette.db import SessionManager\n",
    "from annette.db.models import Citation, ManualClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generic function to get a list of `{'attribute_name': attribute, 'class': classification}` records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(attr_name):\n",
    "    attr = getattr(Citation, attr_name)\n",
    "\n",
    "    with SessionManager() as session_manager:\n",
    "        citations = session_manager.session.query(attr,\n",
    "                                                  ManualClassification.classification_id) \\\n",
    "            .join(ManualClassification, Citation.doi == ManualClassification.doi) \\\n",
    "            .group_by(attr, ManualClassification.classification_id).all()\n",
    "\n",
    "    return [{\n",
    "        attr_name: getattr(c, attr_name) if getattr(c, attr_name) is not None else '',\n",
    "        'class': c.classification_id\n",
    "        } for c in citations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two tokenisers for the `subject` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def tokenise_subjects_simple(subject):\n",
    "    if subject == '' or subject is None:\n",
    "        return []\n",
    "    return list(set([s.strip().lower() for s in subject.split(',')]))\n",
    "\n",
    "\n",
    "def tokenise_subjects_phrases(subject):\n",
    "    if subject == '' or subject is None:\n",
    "        return []\n",
    "    split_subjects = []\n",
    "    phrase_pattern = 'CHUNK:{<JJ>*<NN.?>*<VBG>*}'\n",
    "    phrase_chunker = RegexpParser(phrase_pattern)\n",
    "    for s in subject.split(','):\n",
    "        tokens = word_tokenize(s.strip().lower())\n",
    "        tags = pos_tag(tokens)\n",
    "        phrases = [' '.join([leaf[0] for leaf in c.leaves()]) for c in phrase_chunker.parse(tags) if hasattr(c, 'label') and c.label() == 'CHUNK']\n",
    "        for phrase in phrases:\n",
    "            phrase_tokens = word_tokenize(phrase)\n",
    "            phrase_tags = pos_tag(phrase_tokens)\n",
    "            lemmatised_phrase = []\n",
    "            for pto, pta in phrase_tags:\n",
    "                wn_tag = {\n",
    "                    'n': wn.NOUN,\n",
    "                    'j': wn.ADJ,\n",
    "                    'v': wn.VERB,\n",
    "                    'r': wn.ADV\n",
    "                    }.get(pta[0].lower(), None)\n",
    "                if wn_tag is None:\n",
    "                    continue\n",
    "                lemmatised = WordNetLemmatizer().lemmatize(pto, wn_tag)\n",
    "                lemmatised_phrase.append(lemmatised)\n",
    "            if len(lemmatised_phrase) > 0:\n",
    "                lemmatised_phrase = ' '.join(lemmatised_phrase)\n",
    "                split_subjects.append(lemmatised_phrase)\n",
    "    return list(set(split_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple tokeniser just splits the string by commas (and converts to lowercase). The tags should be fairly standardised, so this shouldn't be too much of a problem.\n",
    "\n",
    "Just to see if it's more accurate/helpful to process the tags in a different way, we've also defined the 'phrase' tokeniser, which goes through several steps:\n",
    "1. Split the whole string by commas (assumes the field is a comma-separated list of phrasal tags);\n",
    "2. Tokenise the phrasal subject tags and identify parts of speech;\n",
    "3. Extract sub-phrases based on those parts of speech (i.e. adjective-noun-verb combinations, ignoring conjunctives like *and*);\n",
    "4. Lemmatise the words in those phrases to reduce differences between related tags (e.g. science and sciences both become *science*);\n",
    "5. Join the words back together to form a lemmatised sub-phrase.\n",
    "\n",
    "The two tokenisers will generate different length lists from the same input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ecology', 'aquatic science', 'nature and landscape conservation']\n",
      "['ecology', 'aquatic science', 'landscape conservation', 'nature']\n"
     ]
    }
   ],
   "source": [
    "print(tokenise_subjects_simple('Ecology,Aquatic Science,Nature and Landscape Conservation'))\n",
    "print(tokenise_subjects_phrases('Ecology,Aquatic Science,Nature and Landscape Conservation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ecology', 'nature and landscape conservation', 'evolution', 'behavior and systematics']\n",
      "['behavior', 'nature', 'systematics', 'ecology', 'evolution', 'landscape conservation']\n"
     ]
    }
   ],
   "source": [
    "print(tokenise_subjects_simple('Ecology,Ecology, Evolution, Behavior and Systematics,Nature and Landscape Conservation'))\n",
    "print(tokenise_subjects_phrases('Ecology,Ecology, Evolution, Behavior and Systematics,Nature and Landscape Conservation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
